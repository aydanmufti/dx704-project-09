{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 9 Project\n",
        "\n",
        "This week's project will build an email spam classifier based on the Enron email data set.\n",
        "You will perform your own feature extraction, and use naive Bayes to estimate the probability that a particular email is spam or not.\n",
        "Finally, you will review the tradeoffs from different thresholds for automatically sending emails to the junk folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBdILvlviZs2"
      },
      "source": [
        "The full project description and a template notebook are available on GitHub: [Project 9 Materials](https://github.com/bu-cds-dx704/dx704-project-09).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRDlXZBVd2aR"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf1nEl0_Khm_"
      },
      "source": [
        "## Part 1: Download Data Set\n",
        "\n",
        "We will be using the Enron spam data set as prepared in this GitHub repository.\n",
        "\n",
        "https://github.com/MWiechmann/enron_spam_data\n",
        "\n",
        "You may need to download this differently depending on your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwpoSzUxKmG9",
        "outputId": "3ace62f1-c32a-462a-d538-36f3638b7dc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-11-01 23:55:30--  https://github.com/MWiechmann/enron_spam_data/raw/refs/heads/master/enron_spam_data.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/MWiechmann/enron_spam_data/refs/heads/master/enron_spam_data.zip [following]\n",
            "--2025-11-01 23:55:30--  https://raw.githubusercontent.com/MWiechmann/enron_spam_data/refs/heads/master/enron_spam_data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15642124 (15M) [application/zip]\n",
            "Saving to: ‘enron_spam_data.zip’\n",
            "\n",
            "enron_spam_data.zip 100%[===================>]  14.92M  88.2MB/s    in 0.2s    \n",
            "\n",
            "2025-11-01 23:55:30 (88.2 MB/s) - ‘enron_spam_data.zip’ saved [15642124/15642124]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/MWiechmann/enron_spam_data/raw/refs/heads/master/enron_spam_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EfCir3ILLv8z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "9gn-4hUzLywO",
        "outputId": "f810652e-7829-44dd-e842-137281f53be2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Message ID</th>\n",
              "      <th>Subject</th>\n",
              "      <th>Message</th>\n",
              "      <th>Spam/Ham</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>christmas tree farm pictures</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>vastar resources , inc .</td>\n",
              "      <td>gary , production from the high island larger ...</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>calpine daily gas nomination</td>\n",
              "      <td>- calpine daily gas nomination 1 . doc</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>re : issue</td>\n",
              "      <td>fyi - see note below - already done .\\nstella\\...</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>meter 7268 nov allocation</td>\n",
              "      <td>fyi .\\n- - - - - - - - - - - - - - - - - - - -...</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33711</th>\n",
              "      <td>33711</td>\n",
              "      <td>= ? iso - 8859 - 1 ? q ? good _ news _ c = eda...</td>\n",
              "      <td>hello , welcome to gigapharm onlinne shop .\\np...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33712</th>\n",
              "      <td>33712</td>\n",
              "      <td>all prescript medicines are on special . to be...</td>\n",
              "      <td>i got it earlier than expected and it was wrap...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33713</th>\n",
              "      <td>33713</td>\n",
              "      <td>the next generation online pharmacy .</td>\n",
              "      <td>are you ready to rock on ? let the man in you ...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33714</th>\n",
              "      <td>33714</td>\n",
              "      <td>bloow in 5 - 10 times the time</td>\n",
              "      <td>learn how to last 5 - 10 times longer in\\nbed ...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33715</th>\n",
              "      <td>33715</td>\n",
              "      <td>dear sir , i am interested in it</td>\n",
              "      <td>hi : )\\ndo you need some softwares ? i can giv...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>33716 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Message ID                                            Subject  \\\n",
              "0               0                       christmas tree farm pictures   \n",
              "1               1                           vastar resources , inc .   \n",
              "2               2                       calpine daily gas nomination   \n",
              "3               3                                         re : issue   \n",
              "4               4                          meter 7268 nov allocation   \n",
              "...           ...                                                ...   \n",
              "33711       33711  = ? iso - 8859 - 1 ? q ? good _ news _ c = eda...   \n",
              "33712       33712  all prescript medicines are on special . to be...   \n",
              "33713       33713              the next generation online pharmacy .   \n",
              "33714       33714                     bloow in 5 - 10 times the time   \n",
              "33715       33715                   dear sir , i am interested in it   \n",
              "\n",
              "                                                 Message Spam/Ham        Date  \n",
              "0                                                    NaN      ham  1999-12-10  \n",
              "1      gary , production from the high island larger ...      ham  1999-12-13  \n",
              "2                 - calpine daily gas nomination 1 . doc      ham  1999-12-14  \n",
              "3      fyi - see note below - already done .\\nstella\\...      ham  1999-12-14  \n",
              "4      fyi .\\n- - - - - - - - - - - - - - - - - - - -...      ham  1999-12-14  \n",
              "...                                                  ...      ...         ...  \n",
              "33711  hello , welcome to gigapharm onlinne shop .\\np...     spam  2005-07-29  \n",
              "33712  i got it earlier than expected and it was wrap...     spam  2005-07-29  \n",
              "33713  are you ready to rock on ? let the man in you ...     spam  2005-07-30  \n",
              "33714  learn how to last 5 - 10 times longer in\\nbed ...     spam  2005-07-30  \n",
              "33715  hi : )\\ndo you need some softwares ? i can giv...     spam  2005-07-31  \n",
              "\n",
              "[33716 rows x 5 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# pandas can read the zip file directly\n",
        "enron_spam_data = pd.read_csv(\"enron_spam_data.zip\")\n",
        "enron_spam_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYypb_fJWF_A",
        "outputId": "17478b00-1c10-4026-a42b-dcc8a368eab5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.5092834262664611)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(enron_spam_data[\"Spam/Ham\"] == \"spam\").mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 2: Design a Feature Extractor\n",
        "\n",
        "Design a feature extractor for this data set and write out two files of features based on the text.\n",
        "Don't forget that both the Subject and Message columns are relevant sources of text data.\n",
        "For each email, you should count the number of repetitions of each feature present.\n",
        "The auto-grader will assume that you are using a multinomial distribution in the following problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-CF6wtn_VRjp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting features from emails:\n",
            "Total emails processed: 33716\n",
            "Sample features from first email: ['word_christmas', 'word_tree', 'word_farm', 'word_pictures', 'dollar_signs', 'exclamation_marks', 'caps_words', 'email_addresses', 'urls', 'length_very_short']\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "# Imports\n",
        "import json\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Extract features from email subject and message\n",
        "# Return a dictionary of feature counts\n",
        "def extract_features(subject, message):\n",
        "    features = Counter()\n",
        "    \n",
        "    # Combine subject and message, handling NaN values\n",
        "    text = \"\"\n",
        "    if pd.notna(subject):\n",
        "        text += str(subject).lower() + \" \"\n",
        "    if pd.notna(message):\n",
        "        text += str(message).lower()\n",
        "    \n",
        "    # Feature 1: Word tokens - alphanumeric sequences\n",
        "    words = re.findall(r'\\b[a-z0-9]+\\b', text)\n",
        "    for word in words:\n",
        "        if len(word) >= 2:  # Skip single characters\n",
        "            features[f\"word_{word}\"] += 1\n",
        "    \n",
        "    # Feature 2: Presence of numbers\n",
        "    if re.search(r'\\d', text):\n",
        "        features[\"has_numbers\"] += 1\n",
        "    \n",
        "    # Feature 3: Presence of dollar signs\n",
        "    features[\"dollar_signs\"] = text.count('$')\n",
        "    \n",
        "    # Feature 4: Presence of exclamation marks\n",
        "    features[\"exclamation_marks\"] = text.count('!')\n",
        "    \n",
        "    # Feature 5: All caps words (usually seen in spam)\n",
        "    if pd.notna(subject):\n",
        "        caps_words = re.findall(r'\\b[A-Z]{2,}\\b', str(subject))\n",
        "        features[\"caps_words\"] = len(caps_words)\n",
        "    \n",
        "    # Feature 6: Special spam-related keywords\n",
        "    spam_keywords = ['free', 'click', 'buy', 'now', 'offer', 'price', \n",
        "                     'discount', 'save', 'order', 'viagra', 'pharmacy',\n",
        "                     'pills', 'medication', 'prescription', 'online']\n",
        "    for keyword in spam_keywords:\n",
        "        if keyword in text:\n",
        "            features[f\"spam_keyword_{keyword}\"] += text.count(keyword)\n",
        "    \n",
        "    # Feature 7: Email-like patterns\n",
        "    features[\"email_addresses\"] = len(re.findall(r'\\b[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}\\b', text))\n",
        "    \n",
        "    # Feature 8: URLs\n",
        "    features[\"urls\"] = len(re.findall(r'http[s]?://|www\\.', text))\n",
        "    \n",
        "    # Feature 9: Message length (binned)\n",
        "    text_length = len(text)\n",
        "    if text_length < 100:\n",
        "        features[\"length_very_short\"] = 1\n",
        "    elif text_length < 500:\n",
        "        features[\"length_short\"] = 1\n",
        "    elif text_length < 1000:\n",
        "        features[\"length_medium\"] = 1\n",
        "    else:\n",
        "        features[\"length_long\"] = 1\n",
        "    \n",
        "    return dict(features)\n",
        "\n",
        "# Apply feature extraction to all emails\n",
        "print(\"Extracting features from emails:\")\n",
        "enron_spam_data['features'] = enron_spam_data.apply(\n",
        "    lambda row: extract_features(row['Subject'], row['Message']), \n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(f\"Total emails processed: {len(enron_spam_data)}\")\n",
        "print(f\"Sample features from first email: {list(enron_spam_data['features'].iloc[0].keys())[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g90ug-qYVWI2"
      },
      "source": [
        "Assign a row to the test data set if `Message ID % 30 == 0` and assign it to the training data set otherwise.\n",
        "Write two files, \"train-features.tsv\" and \"test-features.tsv\" with two columns, Message ID and features_json.\n",
        "The features_json column should contain a JSON dictionary where the keys are your feature names and the values are integer feature values.\n",
        "This will give us a sparse feature representation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t7AjXVlXUpaR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 32592\n",
            "Test set size: 1124\n",
            "Test set proportion: 0.033\n",
            "\n",
            "Files written ------\n",
            "train-features.tsv: 32592 rows\n",
            "test-features.tsv: 1124 rows\n",
            "\n",
            "Sample from train-features.tsv:\n",
            "   Message ID                                      features_json\n",
            "1           1  {\"word_vastar\": 6, \"word_resources\": 4, \"word_...\n",
            "2           2  {\"word_calpine\": 2, \"word_daily\": 2, \"word_gas...\n",
            "3           3  {\"word_re\": 2, \"word_issue\": 4, \"word_fyi\": 1,...\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Split into train and test sets\n",
        "# Test: Message ID % 30 == 0\n",
        "# Train: otherwise\n",
        "enron_spam_data['is_test'] = enron_spam_data['Message ID'] % 30 == 0\n",
        "\n",
        "train_data = enron_spam_data[~enron_spam_data['is_test']].copy()\n",
        "test_data = enron_spam_data[enron_spam_data['is_test']].copy()\n",
        "\n",
        "print(f\"Training set size: {len(train_data)}\")\n",
        "print(f\"Test set size: {len(test_data)}\")\n",
        "print(f\"Test set proportion: {len(test_data) / len(enron_spam_data):.3f}\")\n",
        "\n",
        "# Create the output DataFrames with Message ID and features_json\n",
        "train_output = pd.DataFrame({\n",
        "    'Message ID': train_data['Message ID'],\n",
        "    'features_json': train_data['features'].apply(json.dumps)\n",
        "})\n",
        "\n",
        "test_output = pd.DataFrame({\n",
        "    'Message ID': test_data['Message ID'],\n",
        "    'features_json': test_data['features'].apply(json.dumps)\n",
        "})\n",
        "\n",
        "# Write to TSV files\n",
        "train_output.to_csv('train-features.tsv', sep='\\t', index=False)\n",
        "test_output.to_csv('test-features.tsv', sep='\\t', index=False)\n",
        "\n",
        "print(\"\\nFiles written ------\")\n",
        "print(f\"train-features.tsv: {len(train_output)} rows\")\n",
        "print(f\"test-features.tsv: {len(test_output)} rows\")\n",
        "\n",
        "# Show a sample of the output\n",
        "print(\"\\nSample from train-features.tsv:\")\n",
        "print(train_output.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAEYBd7WUrC0"
      },
      "source": [
        "Submit \"train-features.tsv\" and \"test-features.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwrLU0aIaNB7"
      },
      "source": [
        "Hint: these features will be graded based on the test accuracy of a logistic regression based on the training features.\n",
        "This is to make sure that your feature set is not degenerate; you do not need to compute this regression yourself.\n",
        "You can separately assess your feature quality based on your results in part 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_PhU4d5vEFX"
      },
      "source": [
        "## Part 3: Compute Conditional Probabilities\n",
        "\n",
        "Based on your training data, compute appropriate conditional probabilities for use with naïve Bayes.\n",
        "Use of additive smoothing with $\\alpha=1$ to avoid zeros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3MKi6er-Vde4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training emails: 32592\n",
            "Spam emails: 16599\n",
            "Ham emails: 15993\n",
            "\n",
            "Total unique features (vocabulary size): 154282\n",
            "Total feature counts in spam: 3376073\n",
            "Total feature counts in ham: 4255508\n",
            "\n",
            "Feature probabilities computed: 154282\n",
            "\n",
            "Sample probabilities:\n",
            "            feature  ham_probability  spam_probability\n",
            "0    word_groomsman     2.267682e-07      8.497729e-07\n",
            "1     word_railways     6.803045e-07      5.665153e-07\n",
            "2        word_yyutu     2.267682e-07      5.665153e-07\n",
            "3      word_penises     2.267682e-07      5.665153e-07\n",
            "4         word_yawo     2.267682e-07      5.665153e-07\n",
            "5    word_immingham     2.267682e-07      5.665153e-07\n",
            "6  word_yelpazesine     2.267682e-07      5.665153e-07\n",
            "7       word_averse     1.133841e-06      3.682349e-06\n",
            "8    word_reibstein     4.535363e-07      2.832576e-07\n",
            "9         word_skci     2.267682e-07      1.416288e-06\n",
            "\n",
            "Summary statistics:\n",
            "       ham_probability  spam_probability\n",
            "count     1.542820e+05      1.542820e+05\n",
            "mean      6.481638e-06      6.481638e-06\n",
            "std       1.745699e-04      1.541610e-04\n",
            "min       2.267682e-07      2.832576e-07\n",
            "25%       2.267682e-07      5.665153e-07\n",
            "50%       2.267682e-07      5.665153e-07\n",
            "75%       6.803045e-07      1.416288e-06\n",
            "max       4.008898e-02      2.973157e-02\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Imports\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load the training data with labels\n",
        "train_features = pd.read_csv('train-features.tsv', sep='\\t')\n",
        "train_labels = enron_spam_data[~enron_spam_data['is_test']][['Message ID', 'Spam/Ham']].copy()\n",
        "\n",
        "# Merge features with labels\n",
        "train_data_with_labels = train_features.merge(train_labels, on='Message ID')\n",
        "\n",
        "print(f\"Training emails: {len(train_data_with_labels)}\")\n",
        "print(f\"Spam emails: {(train_data_with_labels['Spam/Ham'] == 'spam').sum()}\")\n",
        "print(f\"Ham emails: {(train_data_with_labels['Spam/Ham'] == 'ham').sum()}\")\n",
        "\n",
        "# Initialize counters for each class\n",
        "spam_feature_counts = defaultdict(int)\n",
        "ham_feature_counts = defaultdict(int)\n",
        "spam_total_features = 0\n",
        "ham_total_features = 0\n",
        "\n",
        "# Count features for each class\n",
        "for idx, row in train_data_with_labels.iterrows():\n",
        "    features = json.loads(row['features_json'])\n",
        "    is_spam = row['Spam/Ham'] == 'spam'\n",
        "    \n",
        "    for feature, count in features.items():\n",
        "        if is_spam:\n",
        "            spam_feature_counts[feature] += count\n",
        "            spam_total_features += count\n",
        "        else:\n",
        "            ham_feature_counts[feature] += count\n",
        "            ham_total_features += count\n",
        "\n",
        "# Get all unique features\n",
        "all_features = set(spam_feature_counts.keys()) | set(ham_feature_counts.keys())\n",
        "vocabulary_size = len(all_features)\n",
        "\n",
        "print(f\"\\nTotal unique features (vocabulary size): {vocabulary_size}\")\n",
        "print(f\"Total feature counts in spam: {spam_total_features}\")\n",
        "print(f\"Total feature counts in ham: {ham_total_features}\")\n",
        "\n",
        "# Compute conditional probabilities with additive smoothing (alpha=1)\n",
        "alpha = 1.0\n",
        "feature_probabilities = []\n",
        "\n",
        "for feature in all_features:\n",
        "    spam_count = spam_feature_counts[feature]\n",
        "    ham_count = ham_feature_counts[feature]\n",
        "    \n",
        "    # P(feature|spam) with Laplace smoothing\n",
        "    spam_prob = (spam_count + alpha) / (spam_total_features + alpha * vocabulary_size)\n",
        "    \n",
        "    # P(feature|ham) with Laplace smoothing\n",
        "    ham_prob = (ham_count + alpha) / (ham_total_features + alpha * vocabulary_size)\n",
        "    \n",
        "    feature_probabilities.append({\n",
        "        'feature': feature,\n",
        "        'ham_probability': ham_prob,\n",
        "        'spam_probability': spam_prob\n",
        "    })\n",
        "\n",
        "# Create a DataFrame\n",
        "prob_df = pd.DataFrame(feature_probabilities)\n",
        "\n",
        "print(f\"\\nFeature probabilities computed: {len(prob_df)}\")\n",
        "print(f\"\\nSample probabilities:\")\n",
        "print(prob_df.head(10))\n",
        "print(f\"\\nSummary statistics:\")\n",
        "print(prob_df[['ham_probability', 'spam_probability']].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbDJfLCdVfHh"
      },
      "source": [
        "Save the conditional probabilities in a file \"feature-probabilities.tsv\" with columns feature, ham_probability and spam_probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kTVFW327VsOC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File saved: feature-probabilities.tsv\n",
            "Total features saved: 154282\n",
            "\n",
            "Features most indicative of spam (high spam_probability):\n",
            "          feature  ham_probability  spam_probability\n",
            "114672   word_the         0.040089          0.029732\n",
            "69690     word_to         0.028540          0.022827\n",
            "95418    word_and         0.018372          0.020315\n",
            "38578     word_of         0.017073          0.019419\n",
            "118151   word_you         0.007582          0.013025\n",
            "113202    word_in         0.013355          0.012586\n",
            "107580  word_this         0.006382          0.009557\n",
            "153678   word_for         0.010937          0.009488\n",
            "45510   word_your         0.002967          0.009263\n",
            "146374    word_is         0.008426          0.009136\n",
            "\n",
            "Features most indicative of ham (high ham_probability):\n",
            "           feature  ham_probability  spam_probability\n",
            "114672    word_the         0.040089          0.029732\n",
            "69690      word_to         0.028540          0.022827\n",
            "95418     word_and         0.018372          0.020315\n",
            "38578      word_of         0.017073          0.019419\n",
            "59311   word_enron         0.013404          0.000001\n",
            "113202     word_in         0.013355          0.012586\n",
            "153678    word_for         0.010937          0.009488\n",
            "154246     word_on         0.009166          0.004899\n",
            "146374     word_is         0.008426          0.009136\n",
            "23584    word_that         0.007827          0.005834\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "# Save to TSV file\n",
        "prob_df.to_csv('feature-probabilities.tsv', sep='\\t', index=False)\n",
        "# Confirm save\n",
        "print(\"File saved: feature-probabilities.tsv\")\n",
        "print(f\"Total features saved: {len(prob_df)}\")\n",
        "\n",
        "# Show some interesting features\n",
        "print(\"\\nFeatures most indicative of spam (high spam_probability):\")\n",
        "print(prob_df.nlargest(10, 'spam_probability')[['feature', 'ham_probability', 'spam_probability']])\n",
        "print(\"\\nFeatures most indicative of ham (high ham_probability):\")\n",
        "print(prob_df.nlargest(10, 'ham_probability')[['feature', 'ham_probability', 'spam_probability']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip-k6K-hVt6q"
      },
      "source": [
        "Submit \"feature-probabilities.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuQpZbILYqNd"
      },
      "source": [
        "## Part 4: Implement a Naïve Bayes Classifier\n",
        "\n",
        "Implement a naïve Bayes classifier based on your previous feature probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jkZeyZgsWr5-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:56: SyntaxWarning: invalid escape sequence '\\A'\n",
            "<>:56: SyntaxWarning: invalid escape sequence '\\A'\n",
            "/tmp/ipykernel_5750/1996048145.py:56: SyntaxWarning: invalid escape sequence '\\A'\n",
            "  print(\"\\Apply naive Bayes classifier to training data ----\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prior probabilities:\n",
            "P(spam) = 0.509297\n",
            "P(ham) = 0.490703\n",
            "\\Apply naive Bayes classifier to training data ----\n",
            "Processed 0 emails...\n",
            "Processed 5000 emails...\n",
            "Processed 10000 emails...\n",
            "Processed 15000 emails...\n",
            "Processed 20000 emails...\n",
            "Processed 25000 emails...\n",
            "Processed 30000 emails...\n",
            "\n",
            "Predictions completed: 32592 emails\n",
            "\n",
            "Sample predictions:\n",
            "   Message ID  ham           spam\n",
            "0           1  1.0  3.007139e-183\n",
            "1           2  1.0   2.285904e-12\n",
            "2           3  1.0  3.839046e-157\n",
            "3           4  1.0  1.940944e-151\n",
            "4           5  1.0   2.952858e-40\n",
            "5           6  1.0   5.167505e-29\n",
            "6           7  1.0  5.569444e-209\n",
            "7           8  1.0   3.788587e-93\n",
            "8           9  1.0  4.269051e-249\n",
            "9          10  1.0   2.829233e-69\n",
            "\n",
            "Summary statistics:\n",
            "                ham          spam\n",
            "count  3.259200e+04  3.259200e+04\n",
            "mean   4.881663e-01  5.118337e-01\n",
            "std    4.977325e-01  4.977325e-01\n",
            "min    0.000000e+00  0.000000e+00\n",
            "25%    7.239133e-38  1.538849e-45\n",
            "50%    7.537969e-03  9.924620e-01\n",
            "75%    1.000000e+00  1.000000e+00\n",
            "max    1.000000e+00  1.000000e+00\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Import numpy\n",
        "import numpy as np\n",
        "\n",
        "# Compute the prior probabilities P(spam) and P(ham)\n",
        "n_spam = (train_data_with_labels['Spam/Ham'] == 'spam').sum()\n",
        "n_ham = (train_data_with_labels['Spam/Ham'] == 'ham').sum()\n",
        "n_total = len(train_data_with_labels)\n",
        "\n",
        "prior_spam = n_spam / n_total\n",
        "prior_ham = n_ham / n_total\n",
        "\n",
        "print(f\"Prior probabilities:\")\n",
        "print(f\"P(spam) = {prior_spam:.6f}\")\n",
        "print(f\"P(ham) = {prior_ham:.6f}\")\n",
        "\n",
        "# Load feature probabilities\n",
        "feature_probs = pd.read_csv('feature-probabilities.tsv', sep='\\t')\n",
        "\n",
        "# Create dictionaries for fast lookup\n",
        "spam_probs_dict = dict(zip(feature_probs['feature'], feature_probs['spam_probability']))\n",
        "ham_probs_dict = dict(zip(feature_probs['feature'], feature_probs['ham_probability']))\n",
        "\n",
        "\n",
        "# Predict the spam/ham probabilities using naive Bayes\n",
        "# Returns (ham_probability, spam_probability)\n",
        "def predict_naive_bayes(features_json):\n",
        "    features = json.loads(features_json)\n",
        "    \n",
        "    # Start with log priors to avoid numerical underflow\n",
        "    log_prob_spam = np.log(prior_spam)\n",
        "    log_prob_ham = np.log(prior_ham)\n",
        "    \n",
        "    # Multiply conditional probabilities (add logs)\n",
        "    for feature, count in features.items():\n",
        "        if feature in spam_probs_dict:\n",
        "            log_prob_spam += count * np.log(spam_probs_dict[feature])\n",
        "        if feature in ham_probs_dict:\n",
        "            log_prob_ham += count * np.log(ham_probs_dict[feature])\n",
        "    \n",
        "    # Convert back from log space and normalize\n",
        "    # Use log-sum-exp trick for numerical stability\n",
        "    max_log_prob = max(log_prob_spam, log_prob_ham)\n",
        "    prob_spam = np.exp(log_prob_spam - max_log_prob)\n",
        "    prob_ham = np.exp(log_prob_ham - max_log_prob)\n",
        "    \n",
        "    # Normalize to sum to 1\n",
        "    total = prob_spam + prob_ham\n",
        "    prob_spam /= total\n",
        "    prob_ham /= total\n",
        "    \n",
        "    return prob_ham, prob_spam\n",
        "\n",
        "# Apply classifier to training data\n",
        "print(\"\\Apply naive Bayes classifier to training data ----\")\n",
        "predictions = []\n",
        "\n",
        "for idx, row in train_features.iterrows():\n",
        "    message_id = row['Message ID']\n",
        "    ham_prob, spam_prob = predict_naive_bayes(row['features_json'])\n",
        "    \n",
        "    predictions.append({\n",
        "        'Message ID': message_id,\n",
        "        'ham': ham_prob,\n",
        "        'spam': spam_prob\n",
        "    })\n",
        "    \n",
        "    if idx % 5000 == 0:\n",
        "        print(f\"Processed {idx} emails...\")\n",
        "\n",
        "predictions_df = pd.DataFrame(predictions)\n",
        "\n",
        "print(f\"\\nPredictions completed: {len(predictions_df)} emails\")\n",
        "print(f\"\\nSample predictions:\")\n",
        "print(predictions_df.head(10))\n",
        "print(f\"\\nSummary statistics:\")\n",
        "print(predictions_df[['ham', 'spam']].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeYGfCYXW89l"
      },
      "source": [
        "Save your prediction probabilities to \"train-predictions.tsv\" with columns Message ID, ham and spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kCKrHbpqZ1gY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File saved: train-predictions.tsv\n",
            "Total predictions saved: 32592\n",
            "\n",
            "Training accuracy: 0.9920\n",
            "\n",
            "Confusion matrix:\n",
            "Predicted    ham   spam\n",
            "Actual                 \n",
            "ham        15805    188\n",
            "spam          73  16526\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "predictions_df.to_csv('train-predictions.tsv', sep='\\t', index=False)\n",
        "print(\"File saved: train-predictions.tsv\")\n",
        "print(f\"Total predictions saved: {len(predictions_df)}\")\n",
        "\n",
        "# Evaluate accuracy on training data\n",
        "train_with_preds = train_data_with_labels.merge(predictions_df, on='Message ID')\n",
        "train_with_preds['predicted_class'] = train_with_preds.apply(\n",
        "    lambda row: 'spam' if row['spam'] > row['ham'] else 'ham', \n",
        "    axis=1\n",
        ")\n",
        "\n",
        "accuracy = (train_with_preds['Spam/Ham'] == train_with_preds['predicted_class']).mean()\n",
        "print(f\"\\nTraining accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Confusion matrix for understanding\n",
        "print(\"\\nConfusion matrix:\")\n",
        "confusion = pd.crosstab(\n",
        "    train_with_preds['Spam/Ham'], \n",
        "    train_with_preds['predicted_class'],\n",
        "    rownames=['Actual'],\n",
        "    colnames=['Predicted']\n",
        ")\n",
        "print(confusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGHYjWN9Z3Sq"
      },
      "source": [
        "Submit \"train-predictions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpTlyFLDOCDj"
      },
      "source": [
        "## Part 5: Predict Spam Probability for Test Data\n",
        "\n",
        "Use your previous classifier to predict spam probability for the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UELHs9CzXaz1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set size: 1124\n",
            "\n",
            "Apply naive Bayes classifier to test data\n",
            "Processed 0 emails...\n",
            "Processed 200 emails...\n",
            "Processed 400 emails...\n",
            "Processed 600 emails...\n",
            "Processed 800 emails...\n",
            "Processed 1000 emails...\n",
            "\n",
            "Predictions completed: 1124 emails\n",
            "\n",
            "Sample predictions:\n",
            "   Message ID       ham           spam\n",
            "0           0  0.042598   9.574016e-01\n",
            "1          30  1.000000   2.612224e-85\n",
            "2          60  1.000000   1.250946e-12\n",
            "3          90  1.000000   8.311318e-34\n",
            "4         120  1.000000  3.588515e-189\n",
            "5         150  1.000000   4.437439e-11\n",
            "6         180  0.999949   5.139716e-05\n",
            "7         210  1.000000   1.058019e-39\n",
            "8         240  1.000000   1.695084e-59\n",
            "9         270  1.000000   3.724895e-39\n",
            "\n",
            "Summary statistics:\n",
            "                ham          spam\n",
            "count  1.124000e+03  1.124000e+03\n",
            "mean   4.847127e-01  5.152873e-01\n",
            "std    4.970497e-01  4.970497e-01\n",
            "min    0.000000e+00  0.000000e+00\n",
            "25%    1.413636e-35  2.038597e-42\n",
            "50%    8.123439e-03  9.918766e-01\n",
            "75%    1.000000e+00  1.000000e+00\n",
            "max    1.000000e+00  1.000000e+00\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "# Load test features from the file we created\n",
        "test_features = pd.read_csv('test-features.tsv', sep='\\t')\n",
        "\n",
        "print(f\"Test set size: {len(test_features)}\")\n",
        "\n",
        "# Apply classifier to test data\n",
        "print(\"\\nApply naive Bayes classifier to test data\")\n",
        "test_predictions = []\n",
        "\n",
        "for idx, row in test_features.iterrows():\n",
        "    message_id = row['Message ID']\n",
        "    ham_prob, spam_prob = predict_naive_bayes(row['features_json'])\n",
        "    \n",
        "    test_predictions.append({\n",
        "        'Message ID': message_id,\n",
        "        'ham': ham_prob,\n",
        "        'spam': spam_prob\n",
        "    })\n",
        "    \n",
        "    if idx % 200 == 0:\n",
        "        print(f\"Processed {idx} emails...\")\n",
        "\n",
        "test_predictions_df = pd.DataFrame(test_predictions)\n",
        "\n",
        "print(f\"\\nPredictions completed: {len(test_predictions_df)} emails\")\n",
        "print(f\"\\nSample predictions:\")\n",
        "print(test_predictions_df.head(10))\n",
        "print(f\"\\nSummary statistics:\")\n",
        "print(test_predictions_df[['ham', 'spam']].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opc86JSEaAQM"
      },
      "source": [
        "Save your prediction probabilities in \"test-predictions.tsv\" with the same columns as \"train-predictions.tsv\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qIg1XaY_Z_Rr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File saved: test-predictions.tsv\n",
            "Total predictions saved: 1124\n",
            "\n",
            "Test accuracy: 0.9831\n",
            "\n",
            "Confusion matrix:\n",
            "Predicted  ham  spam\n",
            "Actual              \n",
            "ham        538    14\n",
            "spam         5   567\n",
            "\n",
            "Test set class distribution:\n",
            "Spam/Ham\n",
            "spam    572\n",
            "ham     552\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Predicted class distribution:\n",
            "predicted_class\n",
            "spam    581\n",
            "ham     543\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Save predictions to TSV\n",
        "test_predictions_df.to_csv('test-predictions.tsv', sep='\\t', index=False)\n",
        "print(\"File saved: test-predictions.tsv\")\n",
        "print(f\"Total predictions saved: {len(test_predictions_df)}\")\n",
        "\n",
        "# Evaluate accuracy on test data\n",
        "test_labels = enron_spam_data[enron_spam_data['is_test']][['Message ID', 'Spam/Ham']].copy()\n",
        "test_with_preds = test_labels.merge(test_predictions_df, on='Message ID')\n",
        "test_with_preds['predicted_class'] = test_with_preds.apply(\n",
        "    lambda row: 'spam' if row['spam'] > row['ham'] else 'ham', \n",
        "    axis=1\n",
        ")\n",
        "\n",
        "test_accuracy = (test_with_preds['Spam/Ham'] == test_with_preds['predicted_class']).mean()\n",
        "print(f\"\\nTest accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion matrix:\")\n",
        "test_confusion = pd.crosstab(\n",
        "    test_with_preds['Spam/Ham'], \n",
        "    test_with_preds['predicted_class'],\n",
        "    rownames=['Actual'],\n",
        "    colnames=['Predicted']\n",
        ")\n",
        "print(test_confusion)\n",
        "\n",
        "# Additional statistics\n",
        "print(\"\\nTest set class distribution:\")\n",
        "print(test_with_preds['Spam/Ham'].value_counts())\n",
        "print(\"\\nPredicted class distribution:\")\n",
        "print(test_with_preds['predicted_class'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLLbyE8paGqM"
      },
      "source": [
        "Submit \"test-predictions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU6ReUMsZNZ8"
      },
      "source": [
        "## Part 6: Construct ROC Curve\n",
        "\n",
        "For every probability threshold from 0.01 to .99 in increments of 0.01, compute the false and true positive rates from the test data using the spam class for positives.\n",
        "That is, if the predicted spam probability is greater than or equal to the threshold, predict spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QAx9jbDBYOVo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compute the ROC curve ------\n",
            "Number of thresholds: 99\n",
            "\n",
            "ROC data computed: 99 thresholds\n",
            "\n",
            "Sample ROC data:\n",
            "   threshold  false_positive_rate  true_positive_rate\n",
            "0       0.01             0.034420            0.996503\n",
            "1       0.02             0.032609            0.996503\n",
            "2       0.03             0.032609            0.996503\n",
            "3       0.04             0.030797            0.996503\n",
            "4       0.05             0.030797            0.996503\n",
            "5       0.06             0.028986            0.996503\n",
            "6       0.07             0.028986            0.996503\n",
            "7       0.08             0.028986            0.996503\n",
            "8       0.09             0.028986            0.996503\n",
            "9       0.10             0.028986            0.996503\n",
            "\n",
            "Summary statistics:\n",
            "       threshold  false_positive_rate  true_positive_rate\n",
            "count  99.000000            99.000000           99.000000\n",
            "mean    0.500000             0.024136            0.989281\n",
            "std     0.287228             0.004115            0.006429\n",
            "min     0.010000             0.014493            0.968531\n",
            "25%     0.255000             0.023551            0.984266\n",
            "50%     0.500000             0.025362            0.991259\n",
            "75%     0.745000             0.025362            0.994755\n",
            "max     0.990000             0.034420            0.996503\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "# We technically already have testing with predictions (test_with_preds) in our previous work\n",
        "# This gave us Message ID, Spam/Ham (actual), ham, spam, (the predicted probabilities)\n",
        "\n",
        "# Generate thresholds from 0.01 to 0.99 in 0.01 increments\n",
        "thresholds = np.arange(0.01, 1.00, 0.01)\n",
        "\n",
        "roc_data = []\n",
        "\n",
        "print(\"Compute the ROC curve ------\")\n",
        "print(f\"Number of thresholds: {len(thresholds)}\")\n",
        "\n",
        "for threshold in thresholds:\n",
        "    # Predict spam if spam probability >= threshold\n",
        "    test_with_preds['predicted_spam'] = test_with_preds['spam'] >= threshold\n",
        "    \n",
        "    # Actual labels (True = spam, False = ham)\n",
        "    actual_spam = test_with_preds['Spam/Ham'] == 'spam'\n",
        "    \n",
        "    # True Positives: actual spam predicted as spam\n",
        "    tp = ((actual_spam) & (test_with_preds['predicted_spam'])).sum()\n",
        "    \n",
        "    # False Positives: actual ham predicted as spam\n",
        "    fp = ((~actual_spam) & (test_with_preds['predicted_spam'])).sum()\n",
        "    \n",
        "    # True Negatives: actual ham predicted as ham\n",
        "    tn = ((~actual_spam) & (~test_with_preds['predicted_spam'])).sum()\n",
        "    \n",
        "    # False Negatives: actual spam predicted as ham\n",
        "    fn = ((actual_spam) & (~test_with_preds['predicted_spam'])).sum()\n",
        "    \n",
        "    # Calculate rates\n",
        "    # True Positive Rate (TPR) = TP / (TP + FN) = TP / Total Actual Positives\n",
        "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    \n",
        "    # False Positive Rate (FPR) = FP / (FP + TN) = FP / Total Actual Negatives\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    \n",
        "    roc_data.append({\n",
        "        'threshold': threshold,\n",
        "        'false_positive_rate': fpr,\n",
        "        'true_positive_rate': tpr\n",
        "    })\n",
        "\n",
        "roc_df = pd.DataFrame(roc_data)\n",
        "\n",
        "print(f\"\\nROC data computed: {len(roc_df)} thresholds\")\n",
        "print(f\"\\nSample ROC data:\")\n",
        "print(roc_df.head(10))\n",
        "print(f\"\\nSummary statistics:\")\n",
        "print(roc_df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baGaDOauX2vE"
      },
      "source": [
        "Save this data in a file \"roc.tsv\" with columns threshold, false_positive_rate and true_positive rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eSHCzA85YP_I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File saved confirmed to: roc.tsv\n",
            "Total thresholds saved: 99\n",
            "\n",
            "ROC data at key thresholds:\n",
            "Threshold 0.01: FPR = 0.0344, TPR = 0.9965\n",
            "Threshold 0.25: FPR = 0.0254, TPR = 0.9948\n",
            "Threshold 0.50: FPR = 0.0254, TPR = 0.9913\n",
            "Threshold 0.75: FPR = 0.0236, TPR = 0.9843\n",
            "Threshold 0.90: FPR = 0.0163, TPR = 0.9808\n",
            "Threshold 0.99: FPR = 0.0145, TPR = 0.9685\n",
            "\n",
            "Area Under ROC Curve (AUC): 0.0197\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "# Save to a TSV file\n",
        "roc_df.to_csv('roc.tsv', sep='\\t', index=False)\n",
        "print(\"File saved confirmed to: roc.tsv\")\n",
        "print(f\"Total thresholds saved: {len(roc_df)}\")\n",
        "\n",
        "# Show some key thresholds\n",
        "print(\"\\nROC data at key thresholds:\")\n",
        "key_thresholds = [0.01, 0.10, 0.25, 0.50, 0.75, 0.90, 0.99]\n",
        "for t in key_thresholds:\n",
        "    row = roc_df[roc_df['threshold'] == t]\n",
        "    if not row.empty:\n",
        "        print(f\"Threshold {t:.2f}: FPR = {row['false_positive_rate'].values[0]:.4f}, TPR = {row['true_positive_rate'].values[0]:.4f}\")\n",
        "\n",
        "# Calculate AUC (Area Under Curve) as a measure of classifier quality\n",
        "from sklearn.metrics import auc\n",
        "auc_score = auc(roc_df['false_positive_rate'], roc_df['true_positive_rate'])\n",
        "print(f\"\\nArea Under ROC Curve (AUC): {auc_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4po8_NMYRuo"
      },
      "source": [
        "Submit \"roc.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynaBbiCZhMYi"
      },
      "source": [
        "## Part 7: Signup for Gemini API Key\n",
        "\n",
        "Create a free Gemini API key at https://aistudio.google.com/app/api-keys.\n",
        "You will need to do this with a personal Google account - it will not work with your BU Google account.\n",
        "This will not incur any charges unless you configure billing information for the key.\n",
        "\n",
        "You will be asked to start a Gemini free trial for week 11.\n",
        "This will not incur any charges unless you exceed expected usage by an order of magnitude.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3xFKcX6hxTL"
      },
      "source": [
        "No submission needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 8: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files.\n",
        "You do not need to provide code for data collection if you did that by manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 9: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File created: acknowledgements.txt\n",
            "\n",
            "Contents:\n",
            "Discussed assignment with:\n",
            "No one\n",
            "\n",
            "Libraries used:\n",
            "- pandas: For data manipulation and reading/writing TSV files\n",
            "- numpy: For numerical computations and log probability calculations\n",
            "- json: For handling JSON-formatted feature dictionaries\n",
            "- re: For regular expression-based text processing and feature extraction\n",
            "- collections: For Counter and defaultdict data structures\n",
            "- matplotlib: For creating ROC curve and error rate visualizations\n",
            "- sklearn.metrics: For computing Area Under Curve (AUC)\n",
            "\n",
            "Additional resources:\n",
            "I used the lecture materials provided in the course, including the project files and lecture videos on naive Bayes classification, conditional probabilities, and ROC curves.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Creating the acknoledgements.txt file using code\n",
        "acknowledgements_text = \"\"\"Discussed assignment with:\n",
        "No one\n",
        "\n",
        "Libraries used:\n",
        "- pandas: For data manipulation and reading/writing TSV files\n",
        "- numpy: For numerical computations and log probability calculations\n",
        "- json: For handling JSON-formatted feature dictionaries\n",
        "- re: For regular expression-based text processing and feature extraction\n",
        "- collections: For Counter and defaultdict data structures\n",
        "- matplotlib: For creating ROC curve and error rate visualizations\n",
        "- sklearn.metrics: For computing Area Under Curve (AUC)\n",
        "\n",
        "Additional resources:\n",
        "I used the lecture materials provided in the course, including the project files and lecture videos on naive Bayes classification, conditional probabilities, and ROC curves.\n",
        "\"\"\"\n",
        "\n",
        "with open('acknowledgements.txt', 'w') as f:\n",
        "    f.write(acknowledgements_text)\n",
        "\n",
        "print(\"File created: acknowledgements.txt\")\n",
        "print(\"\\nContents:\")\n",
        "print(acknowledgements_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
